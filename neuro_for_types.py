import os
os.environ["TORCH_DYNAMO_DISABLE"] = "1"
os.environ["TORCHINDUCTOR_DISABLE"] = "1"

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import re
from collections import Counter
import matplotlib.pyplot as plt
import json

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
MAX_LENGTH = 100
HIDDEN_SIZE = 512
EMBEDDING_SIZE = 256
BATCH_SIZE = 32
EPOCHS = 100
LEARNING_RATE = 0.0005
GRAD_CLIP = 1.0
DROPOUT_RATE = 0.2

# –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø–∏—Å–∫–æ–≤ —Ç–∏–ø–æ–≤
try:
    with open('type_main_list.json', 'r', encoding='utf-8') as f:
        type_main_list = json.load(f)
    with open('type_pod_list.json', 'r', encoding='utf-8') as f:
        type_pod_list = json.load(f)
    print("‚úÖ –¢–∏–ø—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ")
except FileNotFoundError:
    print("‚ö†Ô∏è –§–∞–π–ª—ã —Ç–∏–ø–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Å—Ç—ã–µ —Å–ø–∏—Å–∫–∏")
    type_main_list = []
    type_pod_list = []

class ImprovedTextNormalizer:
    def __init__(self):
        self.word2idx = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}
        self.idx2word = {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>'}
        self.vocab_size = 4

        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç–∏–ø—ã –≤ —Å–ª–æ–≤–∞—Ä—å —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º
        self._add_priority_words()

    def _add_priority_words(self):
        """–î–æ–±–∞–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç–∏–ø—ã –∏ –ø–æ–¥—Ç–∏–ø—ã –≤ —Å–ª–æ–≤–∞—Ä—å"""
        priority_words = set()

        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç–∏–ø—ã
        for main_type in type_main_list:
            if main_type:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Ç—Ä–æ–∫–∞ –Ω–µ –ø—É—Å—Ç–∞—è
                priority_words.add(main_type.lower())
                # –†–∞–∑–±–∏–≤–∞–µ–º —Å–æ—Å—Ç–∞–≤–Ω—ã–µ —Ç–∏–ø—ã
                if '/' in main_type:
                    parts = main_type.split('/')
                    for part in parts:
                        if part.strip():
                            priority_words.add(part.strip().lower())
                if '-' in main_type:
                    parts = main_type.split('-')
                    for part in parts:
                        if part.strip():
                            priority_words.add(part.strip().lower())

        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥—Ç–∏–ø—ã
        for pod_type in type_pod_list:
            if pod_type:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Ç—Ä–æ–∫–∞ –Ω–µ –ø—É—Å—Ç–∞—è
                priority_words.add(pod_type.lower())
                if '-' in pod_type:
                    parts = pod_type.split('-')
                    for part in parts:
                        if part.strip():
                            priority_words.add(part.strip().lower())

        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ —Å–ª–æ–≤–∞ –≤ —Å–ª–æ–≤–∞—Ä—å
        for word in sorted(priority_words):
            if word and word not in self.word2idx:
                self.word2idx[word] = self.vocab_size
                self.idx2word[self.vocab_size] = word
                self.vocab_size += 1

    def build_vocab(self, texts):
        """–°—Ç—Ä–æ–∏–º —Å–ª–æ–≤–∞—Ä—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤"""
        words = []
        for text in texts:
            # –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
            tokens = self._tokenize_text(text)
            words.extend(tokens)

        word_counts = Counter(words)

        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª–æ–≤–∞ –≤ —Å–ª–æ–≤–∞—Ä—å (—Ç–æ–ª—å–∫–æ —Ç–µ, –∫–æ—Ç–æ—Ä—ã—Ö –µ—â–µ –Ω–µ—Ç)
        for word, count in word_counts.items():
            if word not in self.word2idx and count >= 1:
                self.word2idx[word] = self.vocab_size
                self.idx2word[self.vocab_size] = word
                self.vocab_size += 1

    def _tokenize_text(self, text):
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
        # –ó–∞–º–µ–Ω—è–µ–º –∑–∞–ø—è—Ç—ã–µ –∏ —Ç–æ—á–∫–∏ –Ω–∞ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'[.,]', ' ', text)
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞, —Å–æ—Ö—Ä–∞–Ω—è—è –¥–µ—Ñ–∏—Å—ã
        tokens = re.findall(r'[–∞-—èa-z—ë0-9-]+', text.lower())
        return tokens

    def text_to_sequence(self, text):
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å"""
        tokens = self._tokenize_text(text)
        sequence = [self.word2idx.get(token, self.word2idx['<UNK>']) for token in tokens]
        return [self.word2idx['<SOS>']] + sequence + [self.word2idx['<EOS>']]

    def sequence_to_text(self, sequence):
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ —Ç–µ–∫—Å—Ç"""
        tokens = []
        for idx in sequence:
            if idx == self.word2idx['<SOS>']:
                continue
            if idx == self.word2idx['<EOS>']:
                break
            if idx == self.word2idx['<PAD>']:
                continue
            tokens.append(self.idx2word.get(idx, '<UNK>'))
        return ' '.join(tokens)

class Attention(nn.Module):
    def __init__(self, hidden_size):
        super(Attention, self).__init__()
        self.hidden_size = hidden_size
        self.attn = nn.Linear(hidden_size * 3, hidden_size)  # hidden + encoder_outputs (hidden*2)
        self.v = nn.Linear(hidden_size, 1, bias=False)

    def forward(self, hidden, encoder_outputs):
        # hidden: [batch_size, hidden_size] - –∏–∑ –¥–µ–∫–æ–¥–µ—Ä–∞
        # encoder_outputs: [batch_size, seq_len, hidden_size * 2] - –∏–∑ —ç–Ω–∫–æ–¥–µ—Ä–∞

        batch_size = encoder_outputs.shape[0]
        src_len = encoder_outputs.shape[1]

        # Repeat hidden state for every source word
        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)  # [batch_size, src_len, hidden_size]

        # Calculate attention energies
        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # [batch_size, src_len, hidden_size]
        attention = self.v(energy).squeeze(2)  # [batch_size, src_len]

        return torch.softmax(attention, dim=1)

class ImprovedEncoder(nn.Module):
    def __init__(self, vocab_size, embedding_size, hidden_size, dropout_rate):
        super(ImprovedEncoder, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_size)
        self.gru = nn.GRU(embedding_size, hidden_size, batch_first=True,
                         bidirectional=True, num_layers=2, dropout=dropout_rate)
        self.fc = nn.Linear(hidden_size * 2, hidden_size)
        self.dropout = nn.Dropout(dropout_rate)

    def forward(self, x):
        # x: [batch_size, seq_len]
        embedded = self.dropout(self.embedding(x))
        # embedded: [batch_size, seq_len, embedding_size]

        outputs, hidden = self.gru(embedded)
        # outputs: [batch_size, seq_len, hidden_size * 2]
        # hidden: [num_layers * num_directions, batch_size, hidden_size] = [4, batch_size, hidden_size]

        # Combine bidirectional hidden states from last layer
        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–≤–∞ hidden states (forward –∏ backward –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ—è)
        hidden_forward = hidden[-2]  # [batch_size, hidden_size] - forward –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ—è
        hidden_backward = hidden[-1]  # [batch_size, hidden_size] - backward –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ—è

        hidden_combined = torch.tanh(self.fc(
            torch.cat((hidden_forward, hidden_backward), dim=1)
        ))
        # hidden_combined: [batch_size, hidden_size]

        # –î–ª—è –¥–µ–∫–æ–¥–µ—Ä–∞ –Ω—É–∂–Ω–æ –≤–µ—Ä–Ω—É—Ç—å hidden –≤ —Ñ–æ—Ä–º–∞—Ç–µ [num_layers, batch_size, hidden_size]
        # –°–æ–∑–¥–∞–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–π hidden state –¥–ª—è –¥–µ–∫–æ–¥–µ—Ä–∞
        decoder_hidden = hidden_combined.unsqueeze(0)  # [1, batch_size, hidden_size]

        return outputs, decoder_hidden

class ImprovedDecoder(nn.Module):
    def __init__(self, vocab_size, embedding_size, hidden_size, dropout_rate):
        super(ImprovedDecoder, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_size)
        self.attention = Attention(hidden_size)
        self.gru = nn.GRU(embedding_size + hidden_size * 2, hidden_size,
                         batch_first=True, num_layers=1, dropout=0.0)  # –£–±—Ä–∞–ª–∏ dropout –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–ª–æ—è
        self.fc_out = nn.Linear(hidden_size + hidden_size * 2 + embedding_size, vocab_size)
        self.dropout = nn.Dropout(dropout_rate)

    def forward(self, x, hidden, encoder_outputs):
        # x: [batch_size]
        # hidden: [1, batch_size, hidden_size] - –æ—Ç —ç–Ω–∫–æ–¥–µ—Ä–∞
        # encoder_outputs: [batch_size, seq_len, hidden_size * 2]

        x = x.unsqueeze(1)  # [batch_size, 1]
        embedded = self.dropout(self.embedding(x))  # [batch_size, 1, embedding_size]

        # Calculate attention weights
        # hidden: [1, batch_size, hidden_size] -> –±–µ—Ä–µ–º [0] –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è [batch_size, hidden_size]
        attn_weights = self.attention(hidden[0], encoder_outputs)  # [batch_size, src_len]
        attn_weights = attn_weights.unsqueeze(1)  # [batch_size, 1, src_len]

        # Calculate context vector
        context = torch.bmm(attn_weights, encoder_outputs)  # [batch_size, 1, hidden_size * 2]

        # Combine embedded input and context
        gru_input = torch.cat((embedded, context), dim=2)  # [batch_size, 1, embedding_size + hidden_size * 2]

        # GRU forward pass
        output, hidden = self.gru(gru_input, hidden)
        # output: [batch_size, 1, hidden_size]
        # hidden: [1, batch_size, hidden_size]

        # Final prediction
        output_flat = output.squeeze(1)  # [batch_size, hidden_size]
        context_flat = context.squeeze(1)  # [batch_size, hidden_size * 2]
        embedded_flat = embedded.squeeze(1)  # [batch_size, embedding_size]

        combined = torch.cat((output_flat, context_flat, embedded_flat), dim=1)
        prediction = self.fc_out(combined)  # [batch_size, vocab_size]

        return prediction, hidden, attn_weights.squeeze(1)

class ImprovedSeq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device):
        super(ImprovedSeq2Seq, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device

    def forward(self, source, target, teacher_forcing_ratio=0.5):
        batch_size = source.shape[0]
        target_len = target.shape[1]
        target_vocab_size = self.decoder.fc_out.out_features

        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)

        # Encoder forward
        encoder_outputs, hidden = self.encoder(source)

        # First input to decoder is <SOS> token
        x = target[:, 0]

        for t in range(1, target_len):
            output, hidden, _ = self.decoder(x, hidden, encoder_outputs)
            outputs[:, t] = output

            # Teacher forcing
            teacher_force = torch.rand(1).item() < teacher_forcing_ratio
            top1 = output.argmax(1)
            x = target[:, t] if teacher_force else top1

        return outputs

# –î–∞—Ç–∞—Å–µ—Ç
class ImprovedNormalizationDataset(Dataset):
    def __init__(self, source_texts, target_texts, source_normalizer, target_normalizer):
        self.source_texts = source_texts
        self.target_texts = target_texts
        self.source_normalizer = source_normalizer
        self.target_normalizer = target_normalizer

    def __len__(self):
        return len(self.source_texts)

    def __getitem__(self, idx):
        source_seq = self.source_normalizer.text_to_sequence(self.source_texts[idx])
        target_seq = self.target_normalizer.text_to_sequence(self.target_texts[idx])

        source_padded = self.pad_sequence(source_seq, MAX_LENGTH)
        target_padded = self.pad_sequence(target_seq, MAX_LENGTH)

        return torch.tensor(source_padded, dtype=torch.long), torch.tensor(target_padded, dtype=torch.long)

    def pad_sequence(self, sequence, max_length):
        if len(sequence) < max_length:
            return sequence + [0] * (max_length - len(sequence))
        else:
            return sequence[:max_length]

def order_prediction_by_rules(predicted_text):
    """–£–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º: –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–∏–ø –ø–µ—Ä–≤—ã–π, –∑–∞—Ç–µ–º –ø–æ–¥—Ç–∏–ø—ã"""
    words = predicted_text.split()

    if not words:
        return predicted_text

    # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–∏–ø
    main_types_found = []
    pod_types_found = []
    other_words = []

    for word in words:
        word_lower = word.lower()
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–ª–æ–≤–æ –æ—Å–Ω–æ–≤–Ω—ã–º —Ç–∏–ø–æ–º
        is_main_type = any(main_type.lower() == word_lower for main_type in type_main_list)
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–ª–æ–≤–æ –ø–æ–¥—Ç–∏–ø–æ–º
        is_pod_type = any(pod_type.lower() == word_lower for pod_type in type_pod_list)

        if is_main_type:
            main_types_found.append(word)
        elif is_pod_type:
            pod_types_found.append(word)
        else:
            other_words.append(word)

    # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç: –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–∏–ø –ø–µ—Ä–≤—ã–π, –∑–∞—Ç–µ–º –ø–æ–¥—Ç–∏–ø—ã, –∑–∞—Ç–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
    result_parts = []

    # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–∏–ø (–µ—Å–ª–∏ –Ω–∞—à–ª–∏)
    if main_types_found:
        result_parts.append(main_types_found[0])  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–∏–ø
        # –û—Å—Ç–∞–ª—å–Ω—ã–µ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç–∏–ø—ã —Å—á–∏—Ç–∞–µ–º –ø–æ–¥—Ç–∏–ø–∞–º–∏
        pod_types_found.extend(main_types_found[1:])

    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥—Ç–∏–ø—ã
    result_parts.extend(pod_types_found)

    # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
    result_parts.extend(other_words)

    return '|'.join(result_parts)

# –§—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
def train_model(model, dataloader, optimizer, criterion, device):
    model.train()
    epoch_loss = 0

    for i, (source, target) in enumerate(dataloader):
        source, target = source.to(device), target.to(device)

        optimizer.zero_grad()
        output = model(source, target)

        output_dim = output.shape[-1]
        output = output[:, 1:].reshape(-1, output_dim)
        target = target[:, 1:].reshape(-1)

        loss = criterion(output, target)
        loss.backward()

        # Gradient clipping –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=GRAD_CLIP)
        optimizer.step()

        epoch_loss += loss.item()

    return epoch_loss / len(dataloader)

# –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
def improved_predict(model, text, source_normalizer, target_normalizer, device):
    model.eval()

    with torch.no_grad():
        sequence = source_normalizer.text_to_sequence(text)
        if len(sequence) > MAX_LENGTH:
            sequence = sequence[:MAX_LENGTH]
        else:
            sequence = sequence + [0] * (MAX_LENGTH - len(sequence))

        source_tensor = torch.tensor(sequence, dtype=torch.long).unsqueeze(0).to(device)

        encoder_outputs, hidden = model.encoder(source_tensor)

        outputs = [target_normalizer.word2idx['<SOS>']]

        for _ in range(MAX_LENGTH - 1):
            x = torch.tensor([outputs[-1]], dtype=torch.long).to(device)
            output, hidden, _ = model.decoder(x, hidden, encoder_outputs)

            predicted = output.argmax(1).item()
            outputs.append(predicted)

            if predicted == target_normalizer.word2idx['<EOS>']:
                break

        predicted_text = target_normalizer.sequence_to_text(outputs)

        # –ü–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞: —É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–Ω–∏–µ –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º
        return order_prediction_by_rules(predicted_text)

# –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
def safe_load_model(model_path, device):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π custom –∫–ª–∞—Å—Å–æ–≤"""
    try:
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å weights_only=True
        checkpoint = torch.load(model_path, map_location=device, weights_only=True)
        return checkpoint
    except Exception as e:
        print(f"‚ö†Ô∏è Weights-only loading failed: {e}")
        print("üîÑ Trying with safe_globals context...")

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º safe_globals –¥–ª—è —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è TextNormalizer
        with torch.serialization.safe_globals([ImprovedTextNormalizer]):
            checkpoint = torch.load(model_path, map_location=device, weights_only=False)
        return checkpoint

# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
def improved_main(resume_training=False, model_path='improved_type_fixer_model.pth', total_epochs=EPOCHS):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"üöÄ Using device: {device}")

    if device.type == 'cuda':
        print(f"üéØ GPU: {torch.cuda.get_device_name(0)}")
        print(f"üéØ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 3:.1f} GB")
        torch.cuda.empty_cache()

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df = pd.read_excel('types_train.xlsx', sheet_name='river')
    source_texts = df['–ë—ã–ª–æ'].astype(str).tolist()
    target_texts = df['–°—Ç–∞–ª–æ'].astype(str).tolist()

    print(f"üìä Loaded {len(source_texts)} examples")

    # –°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤
    source_normalizer = ImprovedTextNormalizer()
    target_normalizer = ImprovedTextNormalizer()

    source_normalizer.build_vocab(source_texts)
    target_normalizer.build_vocab(target_texts)

    print(f"üìö Source vocabulary size: {source_normalizer.vocab_size}")
    print(f"üìö Target vocabulary size: {target_normalizer.vocab_size}")

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val
    train_source, val_source, train_target, val_target = train_test_split(
        source_texts, target_texts, test_size=0.1, random_state=42
    )

    print(f"üéØ Train size: {len(train_source)}, Val size: {len(val_source)}")

    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    train_dataset = ImprovedNormalizationDataset(train_source, train_target, source_normalizer, target_normalizer)
    val_dataset = ImprovedNormalizationDataset(val_source, val_target, source_normalizer, target_normalizer)

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)

    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    encoder = ImprovedEncoder(source_normalizer.vocab_size, EMBEDDING_SIZE, HIDDEN_SIZE, DROPOUT_RATE)
    decoder = ImprovedDecoder(target_normalizer.vocab_size, EMBEDDING_SIZE, HIDDEN_SIZE, DROPOUT_RATE)
    model = ImprovedSeq2Seq(encoder, decoder, device).to(device)

    # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)

    start_epoch = 0
    train_losses = []

    if resume_training:
        print(f"üîÑ Resuming training from {model_path}")
        checkpoint = safe_load_model(model_path, device)

        if checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
            if 'optimizer_state_dict' in checkpoint:
                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            train_losses = checkpoint.get('train_losses', [])
            start_epoch = checkpoint.get('epoch', len(train_losses))
            print(f"üìÖ Continuing from epoch {start_epoch}")

    print(f"üß† Improved model created with {sum(p.numel() for p in model.parameters()):,} parameters")
    print(f"üéØ Training for {total_epochs - start_epoch} additional epochs (total: {total_epochs})")

    # –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
    criterion = nn.CrossEntropyLoss(ignore_index=0)

    # –û–±—É—á–µ–Ω–∏–µ
    print("üöÄ Starting training...")

    for epoch in range(start_epoch, total_epochs):
        train_loss = train_model(model, train_loader, optimizer, criterion, device)
        train_losses.append(train_loss)

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ learning rate
        scheduler.step(train_loss)

        print(f'üìà Epoch: {epoch + 1:03}/{total_epochs}, Train Loss: {train_loss:.4f}, LR: {optimizer.param_groups[0]["lr"]:.6f}')

        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥—ã–µ 10 —ç–ø–æ—Ö
        if val_source and (epoch + 1) % 10 == 0:
            print(f"üîç Test Results (Epoch {epoch + 1}):")

            test_indices = [
                epoch % len(val_source),
                (epoch + 10) % len(val_source),
                (epoch + 20) % len(val_source)
            ]

            for i, idx in enumerate(test_indices):
                test_text = val_source[idx]
                target_text = val_target[idx]

                prediction = improved_predict(model, test_text, source_normalizer, target_normalizer, device)
                print(f"   Example {i + 1}:")
                print(f"      Input: {test_text}")
                print(f"      Target: {target_text}")
                print(f"      Prediction: {prediction}")
                print('-----------------------------')

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∫–∞–∂–¥—ã–µ 10 —ç–ø–æ—Ö
        if (epoch + 1) % 10 == 0:
            torch.save({
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'source_normalizer': source_normalizer,
                'target_normalizer': target_normalizer,
                'train_losses': train_losses,
                'epoch': epoch + 1
            }, f'improved_type_fixer_model_epoch_{epoch + 1}.pth')
            print(f"üíæ Checkpoint saved: improved_type_fixer_model_epoch_{epoch + 1}.pth")

    # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    torch.save({
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'source_normalizer': source_normalizer,
        'target_normalizer': target_normalizer,
        'train_losses': train_losses,
        'epoch': total_epochs
    }, 'improved_type_fixer_model.pth')

    print("‚úÖ Training completed! Model saved.")

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    plt.figure(figsize=(10, 5))
    plt.plot(train_losses)
    plt.title('Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.savefig('improved_training_loss.png', dpi=300, bbox_inches='tight')
    plt.show()

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
def improved_predict_text(input_text, model_path='improved_type_fixer_model.pth', device=None, pr=False):
    if device is None:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    if pr:
        print(f"üîß Using device: {device}")

    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç –º–æ–¥–µ–ª–∏
        checkpoint = safe_load_model(model_path, device)

        # –ü–æ–ª—É—á–∞–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä—ã –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
        source_normalizer = checkpoint['source_normalizer']
        target_normalizer = checkpoint['target_normalizer']

        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
        encoder = ImprovedEncoder(source_normalizer.vocab_size, EMBEDDING_SIZE, HIDDEN_SIZE, DROPOUT_RATE)
        decoder = ImprovedDecoder(target_normalizer.vocab_size, EMBEDDING_SIZE, HIDDEN_SIZE, DROPOUT_RATE)
        model = ImprovedSeq2Seq(encoder, decoder, device).to(device)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()

        if pr:
            print("‚úÖ Model loaded successfully!")
            print(f"üìö Source vocab size: {source_normalizer.vocab_size}")
            print(f"üìö Target vocab size: {target_normalizer.vocab_size}")

        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        result = improved_predict(model, input_text, source_normalizer, target_normalizer, device)

        if pr:
            print(f"\nüéØ Input: {input_text}")
            print(f"üéØ Output: {result}")

        return result

    except Exception as e:
        print(f"‚ùå Error loading model: {e}")
        print("üí° Make sure the model file exists and is compatible with this code version")
        return None

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
def improved_predict_list(input_list, model_path='improved_type_fixer_model.pth', device=None, pr=False):
    results = []

    if device is None:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    if pr:
        print(f"üîß Using device: {device}")

    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç –º–æ–¥–µ–ª–∏
        checkpoint = safe_load_model(model_path, device)

        # –ü–æ–ª—É—á–∞–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä—ã –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
        source_normalizer = checkpoint['source_normalizer']
        target_normalizer = checkpoint['target_normalizer']

        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
        encoder = ImprovedEncoder(source_normalizer.vocab_size, EMBEDDING_SIZE, HIDDEN_SIZE, DROPOUT_RATE)
        decoder = ImprovedDecoder(target_normalizer.vocab_size, EMBEDDING_SIZE, HIDDEN_SIZE, DROPOUT_RATE)
        model = ImprovedSeq2Seq(encoder, decoder, device).to(device)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()

        if pr:
            print("‚úÖ Model loaded successfully!")
            print(f"üìö Source vocab size: {source_normalizer.vocab_size}")
            print(f"üìö Target vocab size: {target_normalizer.vocab_size}")

        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
        for i, text in enumerate(input_list):
            if i % 10 == 0 and pr:
                print(f"Processing {i}/{len(input_list)}")

            result = improved_predict(model, text, source_normalizer, target_normalizer, device)
            results.append(result)

            if pr:
                print(f"\nüéØ Input: {text}")
                print(f"üéØ Output: {result}")

        return results

    except Exception as e:
        print(f"‚ùå Error loading model: {e}")
        print("üí° Make sure the model file exists and is compatible with this code version")
        return None

# –ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
class ImprovedTypesResult:
    def __init__(self, df, ti):
        self.df = df
        self.ti = ti

    def predicts(self, model_path='improved_type_fixer_model.pth'):
        input_list = self.df.iloc[:, 0].tolist()
        predict_results = improved_predict_list(input_list, model_path=model_path)
        self.df.iloc[:, 1] = predict_results

    def fixing(self):
        for row in range(len(self.df)):
            value0 = str(self.df.iloc[row, 1]).strip()
            # –£–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –≤ order_prediction_by_rules
            self.df.iloc[row, 1] = value0

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è
def continue_improved_training(model_path='improved_type_fixer_model.pth', additional_epochs=50):
    """–ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –º–æ–¥–µ–ª–∏"""
    improved_main(resume_training=True, model_path=model_path, total_epochs=additional_epochs)

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
def test_improved_model(test_list):


    print("üß™ Testing improved model...")
    results = improved_predict_list(test_list)

    print("\nüìã Final Results:")
    for i, (input_text, output_text) in enumerate(zip(test_list, results)):
        print(f"{i+1}. Input: {input_text}")
        print(f"   Output: {output_text}")
        print()

#—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è, –ø—Ä–æ–≤–µ—Ä–∫–∞
def test_tokenizer(test_texts):
    """–¢–µ—Å—Ç–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö"""
    normalizer = ImprovedTextNormalizer()


    print("üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏:")
    print("=" * 50)

    for i, text in enumerate(test_texts, 1):
        tokens = normalizer._tokenize_text(text)
        sequence = normalizer.text_to_sequence(text)
        decoded = normalizer.sequence_to_text(sequence)

        print(f"{i}. –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: {text}")
        print(f"   –¢–æ–∫–µ–Ω—ã: {tokens}")
        print(f"   –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {sequence}")
        print(f"   –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ: {decoded}")
        print(f"   –î–ª–∏–Ω–∞: {len(tokens)} —Ç–æ–∫–µ–Ω–æ–≤")
        print("-" * 30)

def test_vocabulary(test_texts, test_word):
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è"""
    # –°–æ–∑–¥–∞–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä
    normalizer = ImprovedTextNormalizer()

    # –°—Ç—Ä–æ–∏–º —Å–ª–æ–≤–∞—Ä—å
    normalizer.build_vocab(test_texts)

    print("üìö –°–ª–æ–≤–∞—Ä—å:")
    print(f"–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {normalizer.vocab_size}")
    print("\n–°–ª–æ–≤–∞ –≤ —Å–ª–æ–≤–∞—Ä–µ:")
    for i, (word, idx) in enumerate(list(normalizer.word2idx.items())[:]):  # –ü–æ–∫–∞–∂–µ–º –ø–µ—Ä–≤—ã–µ 20
        print(f"  {idx}: '{word}'")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ
    print(f"\nüîÅ –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è:")
    if test_word in normalizer.word2idx:
        idx = normalizer.word2idx[test_word]
        reconstructed = normalizer.idx2word[idx]
        print(f"  '{test_word}' -> {idx} -> '{reconstructed}'")

def test_special_tokens(test_list):
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–±–æ—Ç—É —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤"""
    normalizer = ImprovedTextNormalizer()
    for test_text in test_list:
    #test_text = "—Ç–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç"
      sequence = normalizer.text_to_sequence(test_text)

      print("üéØ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤:")
      print(f"–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: '{test_text}'")
      print(f"–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {sequence}")
      print(f"–ü–µ—Ä–≤—ã–π —Ç–æ–∫–µ–Ω (–¥–æ–ª–∂–µ–Ω –±—ã—Ç—å <SOS>): {normalizer.idx2word.get(sequence[0], 'UNKNOWN')}")
      print(f"–ü–æ—Å–ª–µ–¥–Ω–∏–π —Ç–æ–∫–µ–Ω (–¥–æ–ª–∂–µ–Ω –±—ã—Ç—å <EOS>): {normalizer.idx2word.get(sequence[-1], 'UNKNOWN')}")

      # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
      decoded = normalizer.sequence_to_text(sequence)
      print(f"–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ: '{decoded}'")

def comprehensive_tokenizer_test(test_list):
    """–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞"""
    print("üß™ –ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞")
    print("=" * 50)

    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ —Ç–µ—Å—Ç—ã
    test_special_tokens(test_list)
    print("\n")
    test_tokenizer(test_list)
    #print("\n")
    #test_vocabulary()

def debug_tokenization_issue():
    """–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º—ã —Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–µ–π"""
    normalizer = ImprovedTextNormalizer()

    # –¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç
    test_text = "–ù–µ—Å–∞–º–æ—Ö–æ–¥–Ω–∞—è —Å—É—Ö–æ–≥—Ä—É–∑–Ω–∞—è –º–∞–ª–æ–º–µ—Ä–Ω–æ–µ –±–∞—Ä–∂–∞"

    print("üîç –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏:")
    print("=" * 50)

    # 1. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    tokens = normalizer._tokenize_text(test_text)
    print(f"1. –¢–æ–∫–µ–Ω—ã: {tokens}")

    # 2. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è
    #normalizer.build_vocab([test_text])
    print(f"2. –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {normalizer.vocab_size}")

    # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
    print(f"3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å–ª–æ–≤–∞—Ä–µ:")
    for token in tokens:
        exists = token in normalizer.word2idx
        idx = normalizer.word2idx.get(token, normalizer.word2idx['<UNK>'])
        print(f"   '{token}' -> –≤ —Å–ª–æ–≤–∞—Ä–µ: {exists}, –∏–Ω–¥–µ–∫—Å: {idx}")

    # 4. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
    sequence = normalizer.text_to_sequence(test_text)
    print(f"4. –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {sequence}")

    # 5. –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω–æ
    decoded = normalizer.sequence_to_text(sequence)
    print(f"5. –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ: '{decoded}'")

    # 6. –í—ã–≤–æ–¥ —á–∞—Å—Ç–∏ —Å–ª–æ–≤–∞—Ä—è
    print(f"6. –ß–∞—Å—Ç—å —Å–ª–æ–≤–∞—Ä—è (—Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞):")
    for word, idx in normalizer.word2idx.items():
        if word in tokens or any(token in word for token in tokens):
            print(f"   {idx}: '{word}'")

#



if __name__ == "__main__":
    # –í—ã–±–µ—Ä–∏—Ç–µ –æ–¥–∏–Ω –∏–∑ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤:

    # 1. –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å —Å –Ω—É–ª—è
    #improved_main()

    # 2. –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ
    #continue_improved_training('improved_type_fixer_model.pth', additional_epochs=120)

    test_list = [
        "–ù–µ—Å–∞–º–æ—Ö–æ–¥–Ω–∞—è —Å—É—Ö–æ–≥—Ä—É–∑–Ω–∞—è –º–∞–ª–æ–º–µ—Ä–Ω–æ–µ –±–∞—Ä–∂–∞"
    ]

    # 3. –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å
    #test_improved_model(test_list)

    # —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤
    #comprehensive_tokenizer_test(test_list)
    #test_vocabulary(test_list,"–º–∞–ª–æ–º–µ—Ä–Ω–æ–µ")
    #debug_tokenization_issue()

    # 4. –û–±—Ä–∞–±–æ—Ç–∞—Ç—å Excel —Ñ–∞–π–ª
    i = 0
    file_path = f"types_result_{i}.xlsx"
    file_path_new = f"types_result_{i+1}.xlsx"
    df = (pd.read_excel(file_path, sheet_name=0)).copy()
    tr = ImprovedTypesResult(df, i)
    tr.predicts()
    tr.fixing()
    with pd.ExcelWriter(file_path_new, engine="openpyxl") as writer:
      tr.df.to_excel(writer, sheet_name="result", index=False)